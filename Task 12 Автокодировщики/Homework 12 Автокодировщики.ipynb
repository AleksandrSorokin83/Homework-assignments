{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0719ca86",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Простой-автокодировщик\" data-toc-modified-id=\"Простой-автокодировщик-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Простой автокодировщик</a></span></li><li><span><a href=\"#Автокодировщик-на-сверточной-сети\" data-toc-modified-id=\"Автокодировщик-на-сверточной-сети-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Автокодировщик на сверточной сети</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c660b1",
   "metadata": {},
   "source": [
    "# Простой автокодировщик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945722ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1755f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper params\n",
    "num_epoch = 20\n",
    "cuda_device = -1\n",
    "batch_size = 128\n",
    "device = f'cuda:{cuda_device}' if cuda_device != -1 else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9688c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
      "epoch: 2\n",
      "tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0231, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
      "epoch: 3\n",
      "tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "epoch: 4\n",
      "tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "epoch: 5\n",
      "tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "epoch: 6\n",
      "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "epoch: 7\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "epoch: 8\n",
      "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "epoch: 9\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "epoch: 10\n",
      "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "epoch: 11\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "epoch: 12\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
      "epoch: 13\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "epoch: 14\n",
      "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "epoch: 15\n",
      "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "epoch: 16\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "epoch: 17\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "epoch: 18\n",
      "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "epoch: 19\n",
      "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "epoch: 20\n",
      "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0122, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    # 28*28 -> hidden -> out\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0, 1)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0, 1)\n",
    "        self.linear3 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation(self.linear1(x)))\n",
    "        x = self.dropout2(self.activation(self.linear2(x)))\n",
    "        x = self.activation(self.linear3(x))\n",
    "            \n",
    "        return x\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    # encoder_out -> hidden -> 28*28\n",
    "    def __init__(self, latent_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(0, 1)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(0, 1)\n",
    "        self.linear3 = nn.Linear(hidden_dim, out_dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation(self.linear1(x)))\n",
    "        x = self.dropout2(self.activation(self.linear2(x)))\n",
    "        x = self.activation(self.linear3(x))\n",
    "            \n",
    "        return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def collate_fn(data):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "\n",
    "        pics.append(numpy.array(item[0]))\n",
    "        target.append(item[1])\n",
    "    return {\n",
    "        'data': torch.from_numpy(numpy.array(pics)).float() / 255,\n",
    "        'target': torch.from_numpy(numpy.array(target)),\n",
    "    }\n",
    "\n",
    "# model\n",
    "model = AutoEncoder(28*28, 300, 64)\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "#optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#dataset\n",
    "dataset = datasets.MNIST('/Users/DNS/Documents/Школа Data Scientist/Project/Task 11 MNIST Полносвязная и сверточная сети', download=False)\n",
    "\n",
    "#loss\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "#dataloder\n",
    "for epoch in range(20):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    print(f'epoch: {epoch + 1}')\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        data = batch['data'].to(device).view(batch['data'].size(0), -1)\n",
    "        optim.zero_grad()\n",
    "        predict = model(data)\n",
    "        loss = loss_func(predict, data)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step % 100 == 0):\n",
    "            print(loss)\n",
    "    \n",
    "test = dataset.data[100].view(1,-1).float() / 255\n",
    "predict = model(test)\n",
    "\n",
    "plt.imshow(test[0].view(28,28).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(predict[0].view(28,28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fb0907",
   "metadata": {},
   "source": [
    "# Автокодировщик на сверточной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ebab72",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-754fb7fae3c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-754fb7fae3c5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dim, hidden_dim)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # #conv2d -> maxpool2d -> conv2d -> maxpool2d -> conv2d\n",
    "    def __init__(in_chan, hidden_chan):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_chan, hidden_chan, kernal_size = 5, stride = 1, padding = 2) # 28*28\n",
    "        self.pool1 = MaxPool2d(2, 2) # 14*14\n",
    "        self.conv2 = nn.Conv2d(hidden_chan, hidden_chan, kernal_size = 3, stride = 1, padding = 2) # 14*14\n",
    "        self.pool2 = MaxPool2d(2, 2) # 7*7\n",
    "        self.conv3 = nn.Conv2d(hidden_chan, 1, kernal_size = 3, stride = 1, padding = 2)\n",
    "                \n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x): # 7*7\n",
    "        x = self.activation(self.pool1(self.conv1(x)))\n",
    "        x = self.activation(self.pool2(self.conv2(x)))\n",
    "        x = self.activation(self.conv3(x))\n",
    "            \n",
    "        return x\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    # conv2d -> upsampling2d -> conv2d -> upsampling2d -> conv2d\n",
    "    def __init__(self, in_chan, hidden_chan):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, hidden_chan, kernal_size = 5, stride = 1, padding = 2) # 7*7\n",
    "        self.upsample1 = nn.UpsamplingNearest2d(scale_factor=2) # > 14 x 14\n",
    "        self.conv2 = nn.Conv2d(hidden_ch, hidden_ch, kernel_size=3, stride=1, padding=1)  # > 14 x 14\n",
    "        self.upsample2 = nn.UpsamplingNearest2d(scale_factor=2)  # 28 x 28\n",
    "        self.conv3 = nn.Conv2d(hidden_ch, in_chan, kernel_size=3, stride=1, padding=1)\n",
    "                \n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x): # 28*28\n",
    "        x = self.activation(self.upsample1(self.conv1(x)))\n",
    "        x = self.activation(self.upsample2(self.conv2(x)))\n",
    "        x = self.activation(self.conv3(x))\n",
    "            \n",
    "        return x\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim)\n",
    "        self.decoder = Decoder(input_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def collate_fn(data):\n",
    "    pics = []\n",
    "    target = []\n",
    "    for item in data:\n",
    "\n",
    "        pics.append(numpy.array(item[0]))\n",
    "        target.append(item[1])\n",
    "    return {\n",
    "        'data': torch.from_numpy(numpy.array(pics)).float() / 255,\n",
    "        'target': torch.from_numpy(numpy.array(target)),\n",
    "    }\n",
    "\n",
    "# model\n",
    "model = AutoEncoder(1, 50)\n",
    "model.train()\n",
    "model.to(device)\n",
    "\n",
    "#optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#dataset\n",
    "dataset = datasets.MNIST('/Users/DNS/Documents/Школа Data Scientist/Project/Task 11 MNIST Полносвязная и сверточная сети', download=False)\n",
    "\n",
    "#loss\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "#dataloder\n",
    "for epoch in range(2):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    print(f'epoch: {epoch + 1}')\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        data = batch['data'].to(device).unsqueeze(1)\n",
    "        optim.zero_grad()\n",
    "        predict = model(data)\n",
    "        loss = loss_func(predict, data)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if (step % 100 == 0):\n",
    "            print(loss)\n",
    "    \n",
    "test = dataset.data[100].view(1,-1).float() / 255\n",
    "predict = model(test)\n",
    "\n",
    "plt.imshow(test[0].view(28,28).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(predict[0].view(28,28).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50ccf95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
